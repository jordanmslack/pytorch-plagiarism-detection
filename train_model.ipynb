{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism Detection Model\n",
    "\n",
    "Now that we've created training and test data, we are ready to define and train a model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.pytorch import PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'plagiarism_data'\n",
    "prefix = 'plagiarism_data'\n",
    "\n",
    "data_path = sagemaker_session.upload_data(data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Below is the model that has been designed to identify plagiarism on the dataset created from the previous notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BinaryClassifier\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = torch.load(f)\n",
      "\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = BinaryClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_features\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33moutput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "\n",
      "    model.to(device).eval()\n",
      "    \n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir):\n",
      "    \n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    train_data = pd.read_csv(os.path.join(training_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), header=\u001b[36mNone\u001b[39;49;00m, names=\u001b[36mNone\u001b[39;49;00m)\n",
      "\n",
      "    train_y = torch.from_numpy(train_data[[\u001b[34m0\u001b[39;49;00m]].values).float().squeeze()\n",
      "    train_x = torch.from_numpy(train_data.drop([\u001b[34m0\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m).values).float()\n",
      "\n",
      "    train_ds = torch.utils.data.TensorDataset(train_x, train_y)\n",
      "    data_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m data_loader\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(model, train_loader, epochs, criterion, optimizer, device):\n",
      "    \n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    This is the training method that is called by the PyTorch training script. The parameters\u001b[39;49;00m\n",
      "\u001b[33m    passed are as follows:\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    :param model:\u001b[39;49;00m\n",
      "\u001b[33m        The PyTorch model that we wish to train.\u001b[39;49;00m\n",
      "\u001b[33m    :param train_loader:\u001b[39;49;00m\n",
      "\u001b[33m        The PyTorch DataLoader that should be used during training.\u001b[39;49;00m\n",
      "\u001b[33m    :param epochs:\u001b[39;49;00m\n",
      "\u001b[33m        The total number of epochs to train for.\u001b[39;49;00m\n",
      "\u001b[33m    :param criterion: \u001b[39;49;00m\n",
      "\u001b[33m        The loss function used for training. \u001b[39;49;00m\n",
      "\u001b[33m    :param optimizer:\u001b[39;49;00m\n",
      "\u001b[33m        The optimizer to use during training.\u001b[39;49;00m\n",
      "\u001b[33m    :param device:\u001b[39;49;00m\n",
      "\u001b[33m        Where the model and data should be loaded (gpu or cpu).\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        \n",
      "        model.train()\n",
      "        total_loss = \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mfor\u001b[39;49;00m batch \u001b[35min\u001b[39;49;00m train_loader:\n",
      "            \n",
      "            batch_x, batch_y = batch\n",
      "            batch_x = batch_x.to(device)\n",
      "            batch_y = batch_y.to(device)\n",
      "\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            y_pred = model(batch_x)\n",
      "            \n",
      "            loss = criterion(y_pred, batch_y)\n",
      "            loss.backward()\n",
      "            optimizer.step()\n",
      "            \n",
      "            total_loss += loss.data.item()\n",
      "\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch: {}, Loss: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epoch, total_loss / \u001b[36mlen\u001b[39;49;00m(train_loader)))\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=.\u001b[34m005\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: .005)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input_features\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of initial input features (default:  10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m24\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mH\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of hidden layers in the network (default: 24)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mO\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mdimension of model output (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "\n",
      "    torch.manual_seed(args.seed)\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir)\n",
      "\n",
      "    model = BinaryClassifier(args.input_features, args.hidden_dim, args.output_dim).to(device)\n",
      "\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
      "    criterion = nn.BCELoss()\n",
      "\n",
      "    train(model, train_loader, args.epochs, criterion, optimizer, device)\n",
      "\n",
      "    model_info_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = {\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33minput_features\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.input_features,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.hidden_dim,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33moutput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.output_dim,\n",
      "        }\n",
      "        torch.save(model_info, f)\n",
      "        \n",
      "    model_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        torch.save(model.cpu().state_dict(), f)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir='source',\n",
    "    role=role,\n",
    "    framework_version='1.0',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.xlarge',\n",
    "    hyperparameters={\n",
    "        'input_features': 3,\n",
    "        'hidden_dim': 32,\n",
    "        'output_dim': 1,\n",
    "        'epochs': 50\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Train the estimator\n",
    "\n",
    "Train your estimator on the training data stored in S3. This should create a training job that you can monitor in your SageMaker console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-20 17:59:48 Starting - Starting the training job...\n",
      "2020-02-20 17:59:50 Starting - Launching requested ML instances......\n",
      "2020-02-20 18:00:58 Starting - Preparing the instances for training......\n",
      "2020-02-20 18:02:16 Downloading - Downloading input data\n",
      "2020-02-20 18:02:16 Training - Downloading the training image...\n",
      "2020-02-20 18:02:46 Uploading - Uploading generated training model\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-02-20 18:02:37,260 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-02-20 18:02:37,262 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-20 18:02:37,274 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-02-20 18:02:40,290 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-02-20 18:02:40,700 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-02-20 18:02:40,700 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-02-20 18:02:40,700 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-02-20 18:02:40,700 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5w72f42z/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-02-20 18:02:42,459 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-20 18:02:42,472 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 32,\n",
      "        \"input_features\": 3,\n",
      "        \"epochs\": 50,\n",
      "        \"output_dim\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-02-20-17-59-48-168\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-256376464687/sagemaker-pytorch-2020-02-20-17-59-48-168/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":50,\"hidden_dim\":32,\"input_features\":3,\"output_dim\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-256376464687/sagemaker-pytorch-2020-02-20-17-59-48-168/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":50,\"hidden_dim\":32,\"input_features\":3,\"output_dim\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2020-02-20-17-59-48-168\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-256376464687/sagemaker-pytorch-2020-02-20-17-59-48-168/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"50\",\"--hidden_dim\",\"32\",\"--input_features\",\"3\",\"--output_dim\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=32\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_FEATURES=3\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=50\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIM=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 50 --hidden_dim 32 --input_features 3 --output_dim 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mEpoch: 1, Loss: 0.668849059513637\u001b[0m\n",
      "\u001b[34mEpoch: 2, Loss: 0.646322318485805\u001b[0m\n",
      "\u001b[34mEpoch: 3, Loss: 0.6211045895303998\u001b[0m\n",
      "\u001b[34mEpoch: 4, Loss: 0.6158884423119682\u001b[0m\n",
      "\u001b[34mEpoch: 5, Loss: 0.5488051559243884\u001b[0m\n",
      "\u001b[34mEpoch: 6, Loss: 0.545755365065166\u001b[0m\n",
      "\u001b[34mEpoch: 7, Loss: 0.4959900123732431\u001b[0m\n",
      "\u001b[34mEpoch: 8, Loss: 0.448021480015346\u001b[0m\n",
      "\u001b[34mEpoch: 9, Loss: 0.4558506863457816\u001b[0m\n",
      "\u001b[34mEpoch: 10, Loss: 0.3989548214844295\u001b[0m\n",
      "\u001b[34mEpoch: 11, Loss: 0.4084385803767613\u001b[0m\n",
      "\u001b[34mEpoch: 12, Loss: 0.3703608938625881\u001b[0m\n",
      "\u001b[34mEpoch: 13, Loss: 0.3500116169452667\u001b[0m\n",
      "\u001b[34mEpoch: 14, Loss: 0.2908739298582077\u001b[0m\n",
      "\u001b[34mEpoch: 15, Loss: 0.3255829332130296\u001b[0m\n",
      "\u001b[34mEpoch: 16, Loss: 0.35812188259192873\u001b[0m\n",
      "\u001b[34mEpoch: 17, Loss: 0.29255605063268114\u001b[0m\n",
      "\u001b[34mEpoch: 18, Loss: 0.2650740274361202\u001b[0m\n",
      "\u001b[34mEpoch: 19, Loss: 0.30626367245401653\u001b[0m\n",
      "\u001b[34mEpoch: 20, Loss: 0.2800457051822117\u001b[0m\n",
      "\u001b[34mEpoch: 21, Loss: 0.2617193875568254\u001b[0m\n",
      "\u001b[34mEpoch: 22, Loss: 0.276253525699888\u001b[0m\n",
      "\u001b[34mEpoch: 23, Loss: 0.2610194576638086\u001b[0m\n",
      "\u001b[34mEpoch: 24, Loss: 0.27021595622812\u001b[0m\n",
      "\u001b[34mEpoch: 25, Loss: 0.2614350712725094\u001b[0m\n",
      "\u001b[34mEpoch: 26, Loss: 0.2686067010675158\u001b[0m\n",
      "\u001b[34mEpoch: 27, Loss: 0.2664012142590114\u001b[0m\n",
      "\u001b[34mEpoch: 28, Loss: 0.2830415293574333\u001b[0m\n",
      "\u001b[34mEpoch: 29, Loss: 0.24102392260517394\u001b[0m\n",
      "\u001b[34mEpoch: 30, Loss: 0.2630048170685768\u001b[0m\n",
      "\u001b[34mEpoch: 31, Loss: 0.27293047096048084\u001b[0m\n",
      "\u001b[34mEpoch: 32, Loss: 0.2359751792890685\u001b[0m\n",
      "\u001b[34mEpoch: 33, Loss: 0.2505283515368189\u001b[0m\n",
      "\u001b[34mEpoch: 34, Loss: 0.21875260504228727\u001b[0m\n",
      "\u001b[34mEpoch: 35, Loss: 0.27497220571552006\u001b[0m\n",
      "\u001b[34mEpoch: 36, Loss: 0.2157921471766063\u001b[0m\n",
      "\u001b[34mEpoch: 37, Loss: 0.2577521311385291\u001b[0m\n",
      "\u001b[34mEpoch: 38, Loss: 0.24121062723653658\u001b[0m\n",
      "\u001b[34mEpoch: 39, Loss: 0.24267297770295823\u001b[0m\n",
      "\u001b[34mEpoch: 40, Loss: 0.21216184326580592\u001b[0m\n",
      "\u001b[34mEpoch: 41, Loss: 0.1874176221234458\u001b[0m\n",
      "\u001b[34mEpoch: 42, Loss: 0.20825472154787608\u001b[0m\n",
      "\u001b[34mEpoch: 43, Loss: 0.2375957891345024\u001b[0m\n",
      "\u001b[34mEpoch: 44, Loss: 0.20917440525123052\u001b[0m\n",
      "\u001b[34mEpoch: 45, Loss: 0.23751921632460185\u001b[0m\n",
      "\u001b[34mEpoch: 46, Loss: 0.2680490357535226\u001b[0m\n",
      "\u001b[34mEpoch: 47, Loss: 0.29871813314301626\u001b[0m\n",
      "\u001b[34mEpoch: 48, Loss: 0.24281654294048036\u001b[0m\n",
      "\u001b[34mEpoch: 49, Loss: 0.25942057211484226\u001b[0m\n",
      "\u001b[34mEpoch: 50, Loss: 0.2074497469833919\u001b[0m\n",
      "\u001b[34m2020-02-20 18:02:44,801 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-02-20 18:02:53 Completed - Training job completed\n",
      "Training seconds: 57\n",
      "Billable seconds: 57\n",
      "CPU times: user 473 ms, sys: 13.7 ms, total: 486 ms\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit({'train': os.path.join(data_path, 'train.csv')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------!CPU times: user 467 ms, sys: 14.2 ms, total: 481 ms\n",
      "Wall time: 10min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=estimator.model_data,\n",
    "    role=role,\n",
    "    framework_version='1.0',\n",
    "    entry_point='predict.py',\n",
    "    source_dir='source'\n",
    ")\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "\n",
      "Predicted class labels: \n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "True class labels: \n",
      "[1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "accuracy = accuracy_score(test_y, test_y_preds)\n",
    "print(accuracy)\n",
    "\n",
    "## print out the array of predicted and true labels, if you want\n",
    "print('\\nPredicted class labels: ')\n",
    "print(test_y_preds)\n",
    "print('\\nTrue class labels: ')\n",
    "print(test_y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Observations\n",
    "\n",
    "\n",
    "#### There are no false positives or negatives currently. This could be due to the fact that the data sample is small; it could be, because there is a clear enough line between the answer and source texts that the model was able to correctly label across the board, or there could be some overfitting that has taken place. In reality it is likely a blend of these reasons. \n",
    "\n",
    "#### I chose to create a neural network utilizing PyTorch for a few reasons. I wanted to choose a good solution for the problem, not just a simple solution; and while linear regression models are good for binary classification problems and simpler to create and train, I've found that neural networks that are trained and developed properly can produce more robust results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
